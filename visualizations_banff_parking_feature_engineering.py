# -*- coding: utf-8 -*-
"""Visualizations_Banff_Parking_Feature_Engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13911tdzACJkJC5sjeReyG6tWWoI9FGCH

**Problem Statements:**
- U**tilization Analysis & Feature Identification:** What are the key temporal patterns (hour, day, weekend), historical occupancy trends (lags, rolling averages), weather conditions, and lot-specific characteristics (capacity) that drive hourly parking utilization (number of occupied spots, percentage full) across different Banff lots during the May–September 2025 tourist season?
- **Predictive Modeling:** Using the identified features, can we develop a reliable model to predict future hourly parking demand for specific lots in Banff, forecasting either the expected occupancy count or the probability of a lot reaching near-capacity (>90% full)?
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style
sns.set(style="whitegrid")

# Load the dataset
file_name = 'banff_parking_cleaned_gps.csv'
try:
    df = pd.read_csv(file_name)
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please upload the file to your Colab session.")

# Display the first few rows and column types
if 'df' in locals():
    print(df.head())
    print("\nColumn Info:")
    df.info()

if 'df' in locals() and 'Date' in df.columns:
    min_date = df['Date'].min()
    max_date = df['Date'].max()
    print(f"Date Range: {min_date} to {max_date}")
else:
    print("DataFrame 'df' or 'Date_datetime' column not found.")

# Filter data to include only May 2025 to September 2025

if 'df' in locals():
    # Convert 'Date' column to datetime objects if not already done
    # Using errors='coerce' to handle any potential parsing issues
    df['Date_datetime'] = pd.to_datetime(df['Date'], errors='coerce')

    # Define the start and end dates for filtering
    start_date = '2025-05-01'
    end_date = '2025-09-30'

    # Filter the DataFrame
    df_filtered = df[(df['Date_datetime'] >= start_date) & (df['Date_datetime'] <= end_date)].copy()

    # Replace the original DataFrame with the filtered one
    df = df_filtered

    print(f"DataFrame filtered to include data from {start_date} to {end_date}.")
    print(f"New number of rows: {len(df)}")

else:
    print("DataFrame 'df' not found. Please load the data first.")

# We will focus on data types, especially for dates, and unique values.

if 'df' in locals():
    print("--- Initial Data Review ---")

    # Check unique values in key categorical columns
    print(f"\nUnique 'Zone' values: {df['Zone'].nunique()}")
    print(f"Sample 'Zone' values: {df['Zone'].unique()[:5]}")

    #print(f"\nUnique 'Payment type' values: {df['Payment type'].unique()}")

    print(f"\nUnique 'Type' values: {df['Type'].unique()}")

    # Check date columns - they will need conversion
    # print(f"\nDate columns dtype (before conversion):")
    # print(f"Date: {df['Date'].dtype}")
    print(f"Starting date: {df['Starting date'].dtype}")
    print(f"End date: {df['End date'].dtype}")
else:
    print("DataFrame 'df' not found. Please load the data first.")

!pip install haversine

from haversine import haversine, Unit as haversine_unit

print("\n--- GPS Data Imputation ---")

# 1. Manually define the center coordinates for your known lots
# YOU MUST COMPLETE THIS DICTIONARY YOURSELF
lot_coordinates = {
    'BANFF01- HEALTH UNIT LOT': (51.179, -115.574),
    'BANFF02- WOLF AT MARTEN': (51.178, -115.573),
    'BANFF03- PLAZA PARKADE': (51.179, -115.572),
    'BANFF04- MARTEN 300 BLOCK': (51.181, -115.571),
    'BANFF05- CLOCK TOWER LOT WEST': (51.175, -115.57),
    'BANFF06- ELK STREET IGA': (51.18, -115.571),
    'BANFF07- WOLF HIGH SCHOOL': (51.178, -115.57),
    'BANFF08- FIRE HALL LOT 1 "WEST"': (51.177, -115.57),
    'BANFF09- FIRE HALL LOT "EAST"': (51.177, -115.569),
    'BANFF10- CARIBOU FIRE HALL': (51.177, -115.57),
    'BANFF11- BEAR ST LOT': (51.178, -115.572),
    'BANFF12- CARIBOU MASONS': (51.177, -115.573),
    'BANFF13- MT ROYAL LOT 1 "WEST"': (51.177, -115.57),
    'BANFF14- MT ROYAL LOT 1 "EAST"': (51.176, -115.569),
    'BANFF15- CLOCK TOWER LOT': (51.175, -115.57),
    'BANFF16- BUFFALO AND BANFF': (51.174, -115.571),
    'BANFF17- CENTRAL PARK LOT': (51.174, -115.573),
    'BANFF18- TOWN HALL LOT': (51.175, -115.572),
    'BANFF19- BEAR STREET PARKADE': (51.176, -115.572),
    'BANFF20- LYNX 100 BLOCK': (51.176, -115.573),
    'BANFF21- LYNX 200 BLOCK': (51.177, -115.573),
    'BANFF22- WOLF AT ALLEY': (51.178, -115.572),
    'BANFF23- WOLF 300 SOUTH': (51.178, -115.57),
    'BANFF24- CENTRAL PARK LOT SOUTH': (51.174, -115.573),
    'BANFF25- TOWN HALL LOT NORTH': (51.176, -115.572),
    'BANFF26- FIRE HALL LOT WEST': (51.177, -115.57),
    'BANFF27- BEAR PARKADE L1': (51.176, -115.572),
    'BANFF28- BEAR PARKADE L2': (51.176, -115.572),
}

# 2. Create a function to find the closest lot
def find_closest_lot(row):
    if pd.isna(row['Unit']) and pd.notna(row['Hope Gps Latitude']):
        phone_coords = (row['Hope Gps Latitude'], row['Hope Gps Longitude'])
        distances = {
            unit_name: haversine(phone_coords, unit_coords, unit=haversine_unit.METERS)
            for unit_name, unit_coords in lot_coordinates.items()
        }
        # Find the lot with the minimum distance
        closest_unit = min(distances, key=distances.get)
        return closest_unit
    else:
        # Return the original Unit if it's not null
        return row['Unit']

# 3. Apply this function to impute missing Units
# This may take a moment to run
print(f"Units missing before imputation: {df['Unit'].isna().sum()}")
df['Unit_Imputed'] = df.apply(find_closest_lot, axis=1)
print(f"Units missing after imputation: {df['Unit_Imputed'].isna().sum()}")

# Use the new imputed column as your main 'Unit' column
df['Unit'] = df['Unit_Imputed']
df = df.drop(columns=['Unit_Imputed'])
# ---------------------------------

import folium

# Create a map centered around Banff
# You can adjust the starting coordinates and zoom level as needed
map = folium.Map(location=[51.178, -115.571], zoom_start=14)

# Add markers for each parking lot
# Use the lot_coordinates dictionary defined earlier
if 'lot_coordinates' in locals():
    for lot_name, coords in lot_coordinates.items():
        folium.Marker(
            location=coords,
            popup=lot_name,
            icon=folium.Icon(color='blue', icon='info-sign')
        ).add_to(map)

# Display the map
map

print("\n--- Resampling to Hourly Occupancy ---")

# 1. Get all unique parking lots (Units)
# We dropna() just in case any imputation failed
all_lots = df['Unit'].dropna().unique()

# Convert 'Starting date' and 'End date' to datetime objects with specified format
df['Starting date'] = pd.to_datetime(df['Starting date'], format='%m/%d/%Y %H:%M:%S\u202f%p', errors='coerce')
df['End date'] = pd.to_datetime(df['End date'], format='%m/%d/%Y %H:%M:%S\u202f%p', errors='coerce')

# Drop rows where datetime conversion failed
df.dropna(subset=['Starting date', 'End date'], inplace=True)

# 2. Create a continuous hourly index
hourly_index = pd.date_range(start=df['Starting date'].min(),
                             end=df['End date'].max(),
                             freq='h')

# 3. Create the new hourly DataFrame
hourly_data = []
for lot in all_lots:
    print(f"Processing {lot}...")
    # Use the 'Unit' column (which is now imputed)
    lot_sessions = df[df['Unit'] == lot]
    for hour in hourly_index:
        active_sessions = lot_sessions[
            (lot_sessions['Starting date'] <= hour) &
            (lot_sessions['End date'] > hour)
        ]
        hourly_data.append({
            'Timestamp': hour,
            'Unit': lot,
            'Occupancy': len(active_sessions)
        })

# Create the final hourly DataFrame
df_hourly = pd.DataFrame(hourly_data)
df_hourly = df_hourly.set_index('Timestamp')
# ------------------------

import matplotlib.pyplot as plt
import seaborn as sns

# Time Series Plot: Plot the total occupancy across all lots over time
print("\n--- Daily Total Occupancy Over Time ---")
plt.figure(figsize=(12, 6))
df_hourly['Occupancy'].resample('D').sum().plot()
plt.title('Total Daily Occupancy Across All Lots')
plt.xlabel('Date')
plt.ylabel('Total Occupancy')
plt.show()



# Box Plot by Lot: Show the distribution of Occupancy for each Unit
print("\n--- Hourly Occupancy Distribution by Lot ---")
# Adjust figure size for better readability if many lots
plt.figure(figsize=(15, 8))
sns.boxplot(data=df_hourly, x='Unit', y='Occupancy')
plt.title('Hourly Occupancy Distribution by Lot')
plt.xlabel('Parking Lot (Unit)')
plt.ylabel('Occupancy')
plt.xticks(rotation=90) # Rotate x-axis labels for readability
plt.tight_layout() # Adjust layout to prevent labels overlapping
plt.show()

print("\n--- Adding Features to Hourly Data ---")

# 1. Load and prepare weather data (ensure 'Date' is datetime)
weather_df = pd.read_csv('may_to_august_weather.csv')
# Create a DATE ONLY column for merging
weather_df['Date_Only'] = pd.to_datetime(weather_df['Date/Time']).dt.date
weather_df = weather_df.set_index('Date_Only') # Index by date only

# Ensure df_hourly index is datetime
df_hourly.index = pd.to_datetime(df_hourly.index)
# Create a DATE ONLY column in df_hourly for merging
df_hourly['Date_Only'] = df_hourly.index.date

# Select relevant weather columns to merge (adjust as needed)
weather_cols_to_merge = ['Max Temp (°C)', 'Min Temp (°C)', 'Total Precip (mm)', 'Spd of Max Gust (km/h)'] # Add/remove as needed

# Merge based on the date part
df_hourly = pd.merge(
    df_hourly,
    weather_df[weather_cols_to_merge], # Only merge selected columns
    left_on='Date_Only',
    right_index=True,
    how='left'
)

# Drop the temporary Date_Only column if no longer needed
df_hourly = df_hourly.drop(columns=['Date_Only'])


# Rename specific units to combined names
unit_rename_map = {
    'BANFF18- TOWN HALL LOT': 'Town Hall Lots',
    'BANFF25- TOWN HALL LOT NORTH': 'Town Hall Lots',
    'BANFF08- FIRE HALL LOT 1 "WEST"': 'Fire Hall Lots',
    'BANFF09- FIRE HALL LOT "EAST"': 'Fire Hall Lots',
    'BANFF10- CARIBOU FIRE HALL': 'Fire Hall Lots',
    'BANFF26- FIRE HALL LOT WEST': 'Fire Hall Lots',
    'BANFF19- BEAR STREET PARKADE': 'Bear Street Parkades',
    'BANFF27- BEAR PARKADE L1': 'Bear Street Parkades',
    'BANFF28- BEAR PARKADE L2': 'Bear Street Parkades',
    'BANFF05- CLOCK TOWER LOT WEST' : 'Clock Tower Lots',
    'BANFF15- CLOCK TOWER LOT' : 'Clock Tower Lots',
    'BANFF23- WOLF 300 SOUTH': 'Wolf 300 Block',
    'BANFF07- WOLF HIGH SCHOOL' : 'Wolf 300 Block',
    'BANFF13- MT ROYAL LOT 1 "WEST"': 'Mt Royal Lots',
    'BANFF14- MT ROYAL LOT 1 "EAST"': 'Mt Royal Lots',
    'BANFF17- CENTRAL PARK LOT': 'Central Park Lots',
    'BANFF24- CENTRAL PARK LOT SOUTH': 'Central Park Lots'


}
df_hourly['Unit'] = df_hourly['Unit'].replace(unit_rename_map)

# 2. Temporal Features
df_hourly['Hour'] = df_hourly.index.hour
df_hourly['DayOfWeek'] = df_hourly.index.dayofweek
df_hourly['Month'] = df_hourly.index.month
df_hourly['IsWeekend'] = (df_hourly['DayOfWeek'] >= 5).astype(int)

# 3. Lag Features (Grouped by Unit)
df_hourly['Occupancy_1hr_Ago'] = df_hourly.groupby('Unit')['Occupancy'].shift(1)
df_hourly['Occupancy_24hr_Ago'] = df_hourly.groupby('Unit')['Occupancy'].shift(24)

# 4. Rolling Features
df_hourly['Occupancy_3hr_Roll_Avg'] = df_hourly.groupby('Unit')['Occupancy'].rolling(3).mean().reset_index(0,drop=True)

# 5. Add Classification Target
lot_capacity_map = {
    'BANFF01- HEALTH UNIT LOT': 24, # Updated capacity, from banffparking.ca
    'Town Hall Lots': 29, # Updated capacity for combined lots, from banffparking.ca
    'BANFF02- WOLF AT MARTEN': 10, # from 2024 transportation overview
    'BANFF03- PLAZA PARKADE': 100, # from banffparking.ca
    'BANFF04- MARTEN 300 BLOCK': 21, # from 2024 transportation overview
    'Clock Tower Lots': 48, # Updated capacity for combined lots, from 2024 transportation overview
    'Wolf 300 Block': 12, # for combined lots, from 2024 transportation overview
    'BANFF06- ELK STREET IGA': 8, # from 2024 transportation overview, assuming it's 200 Elk St
    'Fire Hall Lots': 80, # Updated capacity for combined lots
    'BANFF11- BEAR ST LOT': 50, # Updated capacity
    'BANFF12- CARIBOU MASONS': 24, # from 2024 transportation overview, assuming it's all blocks of caribou st
    'Mt Royal Lots': 27, # combined capacity from banffparking.ca
    'BANFF16- BUFFALO AND BANFF': 3, # from 2024 transportation overview
    'Central Park Lots': 78, # Updated capacity for combined lots, from banffparking.ca
    'Bear Street Parkades': 120, # Updated capacity for combined lots, from 2024 transportation overview
    'BANFF20- LYNX 100 BLOCK': 18, # from 2024 banff transportation overview
    'BANFF21- LYNX 200 BLOCK': 18, # from 2024 banff transportation overview
    'BANFF22- WOLF AT ALLEY': 15 # from 2024 banff transportation overview, assuming its 0 wolf st block
}
df_hourly['Capacity'] = df_hourly['Unit'].map(lot_capacity_map)
df_hourly['Percent_Occupancy'] = df_hourly['Occupancy'] / df_hourly['Capacity']
df_hourly['Is_Full'] = (df_hourly['Percent_Occupancy'] > 0.90).astype(int) # Target is 90% full

# Check weather columns specifically
print("Number of NaNs per column AFTER weather merge (before lag/roll drop):")
print(df_hourly.isna().sum())

# Forward fill weather NaNs
weather_cols_merged = [col for col in weather_cols_to_merge if col in df_hourly.columns] # Get actual merged columns
if weather_cols_merged:
     df_hourly[weather_cols_merged] = df_hourly[weather_cols_merged].fillna(method='ffill').fillna(method='bfill') # Forward then backward fill

# NOW handle NaNs from lags/rolls by dropping rows
print(f"Number of rows before lag/roll dropna: {len(df_hourly)}")
df_hourly = df_hourly.dropna(subset=['Occupancy_1hr_Ago', 'Occupancy_24hr_Ago', 'Occupancy_3hr_Roll_Avg'])
print(f"Number of rows AFTER lag/roll dropna: {len(df_hourly)}")

# 6. Save the final dataset
df_hourly.to_csv('banff_parking_engineered_HOURLY.csv', index=True)
print("Successfully saved hourly engineered data.")
# ---------------------------------



import matplotlib.pyplot as plt
import seaborn as sns

print("\n--- Additional Feature Visualizations (Hourly Data) ---")

# Bar Plots/Box Plots: Show average Occupancy or Percent_Occupancy by Hour, DayOfWeek, or IsWeekend
# Average Occupancy by Hour
plt.figure(figsize=(10, 6))
sns.barplot(data=df_hourly, x='Hour', y='Occupancy')
plt.title('Average Hourly Occupancy')
plt.xlabel('Hour of Day')
plt.ylabel('Average Occupancy')
plt.show()

# Average Percent_Occupancy by DayOfWeek
plt.figure(figsize=(10, 6))
sns.barplot(data=df_hourly, x='DayOfWeek', y='Percent_Occupancy')
plt.title('Average Percent Occupancy by Day of Week')
plt.xlabel('Day of Week (0=Monday, 6=Sunday)')
plt.ylabel('Average Percent Occupancy')
plt.show()

# Average Occupancy by IsWeekend
plt.figure(figsize=(8, 6))
sns.boxplot(data=df_hourly, x='IsWeekend', y='Occupancy')
plt.title('Hourly Occupancy Distribution: Weekday vs. Weekend')
plt.xlabel('Is Weekend (0=No, 1=Yes)')
plt.ylabel('Occupancy')
plt.show()


# Histograms: Check the distributions of weather features
weather_features = ['Max Temp (°C)', 'Min Temp (°C)', 'Total Precip (mm)', 'Spd of Max Gust (km/h)']
print("\n--- Distribution of Weather Features ---")
plt.figure(figsize=(15, 5))
for i, feature in enumerate(weather_features):
    if feature in df_hourly.columns:
        plt.subplot(1, len(weather_features), i + 1)
        sns.histplot(data=df_hourly, x=feature, kde=True, bins=20)
        plt.title(f'Distribution of {feature}')
        plt.xlabel(feature)
        plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Line Plots: Plot average Occupancy vs. Hour for weekdays vs. weekends
print("\n--- Average Hourly Occupancy: Weekday vs. Weekend ---")
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_hourly, x='Hour', y='Occupancy', hue='IsWeekend')
plt.title('Average Hourly Occupancy: Weekday vs. Weekend')
plt.xlabel('Hour of Day')
plt.ylabel('Average Occupancy')
plt.legend(title='Is Weekend', labels=['Weekday', 'Weekend'])
plt.show()

# Drop columns that are completely full of null values
if 'df' in locals():
    # Select columns with all null values
    cols_with_all_nulls = df.columns[df.isnull().all()]

    if len(cols_with_all_nulls) > 0:
        df = df.drop(columns=cols_with_all_nulls)
        print(f"Dropped columns with all null values: {list(cols_with_all_nulls)}")
        print("\nDataFrame after dropping columns:")
        print(df.head())
        print("\nColumn Info after dropping:")
        df.info()
    else:
        print("No columns with all null values found.")
else:
    print("DataFrame 'df' not found. Please load the data first.")

# This block performs correlation analysis
print("\n--- Feature Relationship Analysis (Hourly Data) ---")

# Choose your target variable for prediction
TARGET_VARIABLE = 'Percent_Occupancy'

# Select numeric features from df_hourly for correlation
# Ensure weather columns added previously are included if numeric
hourly_numeric_features = df_hourly.select_dtypes(include=np.number).columns.tolist()

# Remove features that might cause multicollinearity or aren't predictors (e.g., Capacity if predicting Percent_Occupancy)
features_to_exclude = ['Capacity'] # Add others if needed
hourly_numeric_features = [f for f in hourly_numeric_features if f not in features_to_exclude]

if TARGET_VARIABLE not in hourly_numeric_features:
     print(f"Target variable '{TARGET_VARIABLE}' is not numeric or not found.")
else:
    # --- Start: Memory Optimization - Sampling df_hourly ---
    sample_size_hourly = min(100000, len(df_hourly))
    df_hourly_sampled = df_hourly.sample(n=sample_size_hourly, random_state=42)
    print(f"Analyzing a sampled subset of the hourly data (n={sample_size_hourly})...")
    # --- End: Memory Optimization ---

    # Calculate correlation on sampled hourly data
    corr_matrix_hourly = df_hourly_sampled[hourly_numeric_features].corr()

    plt.figure(figsize=(12, 10)) # Adjust size as needed
    sns.heatmap(corr_matrix_hourly, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
    plt.title('Hourly Feature Correlation Matrix (Sampled Data)')
    plt.show()

    # Show correlations with the target variable
    print(f"\n--- Correlations with Target Variable ({TARGET_VARIABLE}) ---")
    target_correlations_hourly = corr_matrix_hourly[TARGET_VARIABLE].sort_values(ascending=False)
    print(target_correlations_hourly)

    # Scatter Plots for Hourly Data
    print(f"\n--- Scatter Plots vs. Target ({TARGET_VARIABLE}) (Sampled Hourly Data) ---")
    features_to_plot = ['Hour', 'DayOfWeek', 'Occupancy_1hr_Ago', 'Occupancy_24hr_Ago'] # Choose relevant features
    # Add merged weather columns if desired, e.g., 'Max Temp (°C)'

    num_plots = len(features_to_plot)
    plt.figure(figsize=(6 * num_plots, 5))

    for i, feature in enumerate(features_to_plot):
         if feature in df_hourly_sampled.columns:
             plt.subplot(1, num_plots, i + 1)
             sns.scatterplot(data=df_hourly_sampled, x=feature, y=TARGET_VARIABLE, alpha=0.3)
             plt.title(f'{TARGET_VARIABLE} vs. {feature}')

    plt.tight_layout()
    plt.show()

!pip install openpyxl

# Save the DataFrame with engineered features to a new Excel file
output_file_name = 'banff_parking_engineered_HOURLY.xlsx'

try:
    # Use to_excel() to save to an Excel file
    df_hourly.to_excel(output_file_name, index=True) # index=True to keep the timestamp index
    print(f"DataFrame successfully saved to '{output_file_name}'")
except Exception as e:
    print(f"Error saving DataFrame to Excel: {e}")

"""# Visualizations"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

"""Average Occupancy Heatmap (Hour vs Day of Week)"""

# 1. Create the pivot table
# We calculate the average 'Occupancy_Percent' for each cell
pivot_table = df_hourly.pivot_table(
    values='Percent_Occupancy',
    index='Hour',
    columns='DayOfWeek',
    aggfunc='mean'
)

# 2. (Optional) Order the columns if they are strings
try:
    day_order = [
        'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
    ]
    # Map the numeric DayOfWeek to names for plotting
    pivot_table.columns = [day_order[col] for col in pivot_table.columns]
    pivot_table = pivot_table[day_order] # Ensure correct order
except (KeyError, IndexError):
    # If it fails, they are likely already numbers (0-6) and sorted, which is fine.
    pass

# 3. Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(
    pivot_table,
    annot=True,     # Add numbers to the cells
    fmt=".0%",      # Format numbers as percentages (e.g., "85%")
    cmap='viridis', # A nice color map (yellow=high, blue=low)
    linewidths=.5   # Add lines between cells
)
plt.title('Average Parking Occupancy (%) by Hour and Day of Week', fontsize=16)
plt.xlabel('Day of Week')
plt.ylabel('Hour of Day')
plt.show()

"""Box Plots for Temporal Features"""

# Set up the figure for two plots
fig, axes = plt.subplots(1, 2, figsize=(18, 7))

# Plot 1: Occupancy Distribution by Hour
sns.boxplot(
    data=df_hourly,
    x='Hour',
    y='Percent_Occupancy',
    ax=axes[0]
)
axes[0].set_title('Occupancy Distribution by Hour of Day')
axes[0].set_xlabel('Hour')
axes[0].set_ylabel('Occupancy Percent')

# Plot 2: Occupancy Distribution by Day of Week
# (Optional) Order the days if 'DayOfWeek' is a string
day_order = [
    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
]
sns.boxplot(
    data=df_hourly,
    x='DayOfWeek',
    y='Percent_Occupancy',
    ax=axes[1]
)
axes[1].set_title('Occupancy Distribution by Day of Week')
axes[1].set_xlabel('Day of Week (0=Monday, 6=Sunday)')
axes[1].set_ylabel('Occupancy Percent')

plt.tight_layout()
plt.show()

"""Correlation Heatmap"""

# This block performs correlation analysis
print("\n--- Feature Relationship Analysis (Hourly Data) ---")

# Choose your target variable for prediction
TARGET_VARIABLE = 'Percent_Occupancy'

# Select numeric features from df_hourly for correlation
# Ensure weather columns added previously are included if numeric
# Update the feature list to match the columns in df_hourly
numerical_features = [
    'Occupancy', # Direct occupancy count
    TARGET_VARIABLE, # The target variable
    'Occupancy_1hr_Ago',   # lag feature
    'Occupancy_24hr_Ago',  # lag feature
    'Occupancy_3hr_Roll_Avg', # rolling average feature
    'Max Temp (°C)',       # weather feature
    'Min Temp (°C)',       # weather feature
    'Total Precip (mm)',   # weather feature
    'Spd of Max Gust (km/h)',# weather feature
    'Hour',
    'DayOfWeek',           # numeric version (0-6)
    'Month',
    'IsWeekend'            # boolean/int feature
]

# Create a mini-DataFrame with just these features
# Use df_hourly instead of df
df_corr = df_hourly[df_hourly.columns.intersection(numerical_features)].corr()

# Show correlations with the target variable
if TARGET_VARIABLE in df_corr.columns:
     print(f"\n--- Correlations with Target Variable ({TARGET_VARIABLE}) ---")
     target_correlations_hourly = df_corr[TARGET_VARIABLE].sort_values(ascending=False)
     print(target_correlations_hourly)

     # Filter correlations based on a threshold (e.g., absolute correlation > 0.1)
     correlation_threshold = 0.1
     important_features = target_correlations_hourly[abs(target_correlations_hourly) > correlation_threshold].index.tolist()

     # Filter the correlation matrix to include only important features and the target
     important_corr_matrix = df_corr.loc[important_features, important_features]


     # Plot the heatmap of important correlations
     plt.figure(figsize=(8, 6)) # Adjust size as needed for fewer features
     sns.heatmap(
        important_corr_matrix,
        annot=True,         # Show the correlation numbers
        cmap='coolwarm',    # Red=positive, Blue=negative
        fmt=".2f",          # Show two decimal places
        vmin=-1, vmax=1     # Lock the scale from -1 to 1
     )
     plt.title(f'Important Feature Correlation Heatmap (Abs Correlation > {correlation_threshold} with {TARGET_VARIABLE})')
     plt.show()

else:
     print(f"\nTarget variable '{TARGET_VARIABLE}' not found in the correlation matrix.")

"""Faceted Plots (Average Hourly Profile by Lot)"""

# Use seaborn's 'relplot' to create a faceted line plot
# 'col="Unit"' tells seaborn to create a new column of plots for each unique value
# 'col_wrap=3' means it will wrap to a new row after 3 plots
g = sns.relplot(
    data=df_hourly, # Use df_hourly
    x='Hour',
    y='Percent_Occupancy', # Use correct column name
    col='Unit',      # Use correct column name for lots
    col_wrap=4,          # Arrange plots in a grid, adjust wrap count as needed
    kind='line',         # Make it a line plot
    height=3, # Adjust height as needed
    aspect=1.0, # Adjust aspect as needed
    ci=None              # Turn off confidence interval for a cleaner average line
)

g.fig.suptitle('Average Hourly Occupancy Profile by Lot', y=1.03, fontsize=16)
g.set_axis_labels('Hour of Day', 'Average Occupancy %')
plt.show()

"""Feature Importance Plot (After Modeling)"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# --- ASSUMPTIONS ---
# This is a self-contained example.
# You will need to adapt 'features' and 'target' to your project.

# 1. Define your features (X) and target (y)
# Replace these lists with your actual feature columns!
# Make sure to drop non-numeric columns and the target variable itself.
# Updated feature columns based on df_hourly
feature_cols = [
    'Occupancy_1hr_Ago',   # lag feature
    'Occupancy_24hr_Ago',  # lag feature
    'Occupancy_3hr_Roll_Avg', # rolling average feature
    'Max Temp (°C)',       # weather feature
    'Min Temp (°C)',       # weather feature
    'Total Precip (mm)',   # weather feature
    'Spd of Max Gust (km/h)',# weather feature
    'Hour',
    'DayOfWeek',           # numeric version (0-6)
    'Month',
    'IsWeekend'            # boolean/int feature
    # Add 'Unit' if you plan to use categorical features (requires encoding)
    # For simplicity, we'll start with numeric features only
]
target_col = 'Percent_Occupancy'

# Select only features that actually exist in df_hourly
valid_features = [col for col in feature_cols if col in df_hourly.columns]

if not valid_features:
    print("Error: None of the assumed feature columns were found in your DataFrame.")
    print("Please update the 'feature_cols' list in the script.")
else:
    # Create X and y, dropping any rows with missing values in the selected columns
    # Use df_hourly instead of df
    df_model = df_hourly[valid_features + [target_col]].dropna()

    X = df_model[valid_features]
    y = df_model[target_col]

    # 2. Split data (optional but good practice)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 3. Train a simple model
    # n_estimators=50 is small for speed, you might use 100+
    model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    print("Model trained successfully.")

    # 4. Get and plot feature importances
    importances = model.feature_importances_

    # Create a DataFrame for easy plotting
    importance_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    # 5. Plot the bar chart
    plt.figure(figsize=(10, 6))
    sns.barplot(
        data=importance_df,
        x='Importance',
        y='Feature'
    )
    plt.title('Feature Importances from Random Forest Model')
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.show()