# -*- coding: utf-8 -*-
"""visits_dataset _group3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12j4gZ6iJml-rYLSQFw04r5dNv17V9Rpu
"""

#Block 1: Loading and Overview of the Dataset (20%))

plt.figure(figsize=(7, 4))
sns.barplot(data=site_counts, x='site', y='total_site_count', hue='site', palette='viridis', legend=False)
plt.title("Total Number of Vehicles per Site")
plt.xlabel("Parking Site")
plt.ylabel("Total Vehicles")
plt.xticks(rotation=45)
plt.show()

import pandas as pd

# Load dataset
filename = "visits-2025-04-29 13_34_54.978523+00_00.csv"
df = pd.read_csv(filename)

# Show first 5 rows
print("First 5 rows:")
display(df.head())

# Show data types explicitly
print("\nData types:")
print(df.dtypes)

# Dataset info
print("\nDataset info:")
df.info()

# Missing values check
print("\nMissing values per column:")
print(df.isnull().sum())

#Block 1: Loading and Overview of the Dataset
#What are we doing:

#Loading the dataset into a pandas DataFrame.

#Looking at the first few rows to get a sense of the data.

#Checking column names, data types, and how many missing values each column has.

#Why this matters:This helps us understand:

#What kind of data weâ€™re dealing with (text, numbers, dates, etc.).

#If there are any obvious problems (e.g. missing or weird values).

#The overall shape and quality of the data.

#Block 2: Data Cleaning and Preparation

# Drop duplicates
df.drop_duplicates(inplace=True)

# Convert date columns with format MM/DD/YYYY (e.g. 4/27/2025)
df['entrance_date'] = pd.to_datetime(df['entrance_date'], format='%m/%d/%Y', errors='coerce')
df['exit_date'] = pd.to_datetime(df['exit_date'], format='%m/%d/%Y', errors='coerce')

# Combine date and time into datetime columns
df['entrance_datetime'] = pd.to_datetime(
    df['entrance_date'].dt.strftime('%Y-%m-%d') + ' ' + df['entrance_time'],
    errors='coerce'
)
df['exit_datetime'] = pd.to_datetime(
    df['exit_date'].dt.strftime('%Y-%m-%d') + ' ' + df['exit_time'],
    errors='coerce'
)

# Calculate visit duration in minutes from entrance and exit datetime
df['computed_duration_min'] = (df['exit_datetime'] - df['entrance_datetime']).dt.total_seconds() / 60

# Set invalid durations (negative or missing) to NaN
df.loc[df['computed_duration_min'] < 0, 'computed_duration_min'] = pd.NA

# Print shape and preview cleaned data
print("\nDataset shape after cleaning:", df.shape)
display(df.head())

#What we are doing:

#Handling missing values (either fill them in or drop them).

#Converting date columns like entrance_date and entrance_time into proper datetime format so we can do time-based analysis.

#Removing duplicate rows if any exist.

#Creating new useful columns, such as a full entrance_datetime.

#Converting the duration (which is stored as HH:MM:SS strings) into numeric values in minutes.
# Why this matters:

#Cleaning the data helps prevent errors during analysis. For example:

#We can't plot or calculate stats on strings like '0:43:24'â€”you must convert it to a number (like 43.4 minutes).

#Dates must be in datetime format to group by day, month, hour, etc.

#Block 3: Descriptive Statistics

# Deduplicate based on license_plate (unique vehicles)
df_unique = df.drop_duplicates(subset=['license_plate'])

# Numerical columns for summary stats
numerical_cols = ['duration', 'plate_score', 'region_score', 'computed_duration_min']

print("Summary statistics for numerical columns:")
display(df_unique[numerical_cols].describe())

# Categorical columns to inspect
categorical_cols = ['region', 'type', 'color']

for col in categorical_cols:
    print(f"\nValue counts for {col}:")
    display(df_unique[col].value_counts())

# Missing values check post deduplication
print("\nMissing values after deduplication:")
print(df_unique.isnull().sum())

#What we are doing:

#Categorizing columns as either numeric (like duration) or categorical (like car type, color).

#Removing duplicate license plates to analyze unique visitors.

#Showing descriptive stats like mean, median, min, and max for numerical columns.

#Showing frequency counts of categories like car make, model, and type.

#Why this matters:

#Descriptive stats give a quick summary:

#Whatâ€™s the average visit duration?

#Which types of cars are most common?

#Are some cameras or times more active than others?

df_unique.head()

# Keep only relevant columns
keep_cols = [
    'entrance_datetime', 'exit_datetime', 'computed_duration_min',
    'license_plate', 'region', 'site', 'make', 'model', 'type', 'color'
]

df_cleaned = df_unique[keep_cols].copy()

print("âœ… Columns after cleanup:", df_cleaned.columns.tolist())
print("Shape after cleanup:", df_cleaned.shape)

df_cleaned.isna().sum()

# Convert to datetime first (if not already)
df_cleaned['entrance_datetime'] = pd.to_datetime(df_cleaned['entrance_datetime'], errors='coerce')
df_cleaned['exit_datetime'] = pd.to_datetime(df_cleaned['exit_datetime'], errors='coerce')

# 1ï¸âƒ£ Fill missing datetime values using forward fill (keeps time continuity)
df_cleaned['entrance_datetime'] = df_cleaned['entrance_datetime'].fillna(method='ffill').fillna(method='bfill')
df_cleaned['exit_datetime'] = df_cleaned['exit_datetime'].fillna(method='ffill').fillna(method='bfill')

# 2ï¸âƒ£ Recalculate duration if both timestamps exist
mask = df_cleaned['computed_duration_min'].isna()
df_cleaned.loc[mask, 'computed_duration_min'] = (
    (df_cleaned.loc[mask, 'exit_datetime'] - df_cleaned.loc[mask, 'entrance_datetime']).dt.total_seconds() / 60
)

# 3ï¸âƒ£ Fill any still-missing durations with the overall median
df_cleaned['computed_duration_min'] = df_cleaned['computed_duration_min'].fillna(df_cleaned['computed_duration_min'].median())

# 4ï¸âƒ£ Fill missing 'site' using the most frequent site in the dataset
df_cleaned['site'] = df_cleaned['site'].fillna(df_cleaned['site'].mode()[0])

# âœ… Check again
print("âœ… Missing values after fixing:")
print(df_cleaned.isna().sum())

for col in df_cleaned.columns:
    print(f"ðŸ”¹ Unique values in '{col}':")
    if df_cleaned[col].dtype == 'datetime64[ns]':
        # Displaying unique datetimes might be too verbose,
        # so we'll just show the number of unique values and a few examples
        print(f"Number of unique values: {df_cleaned[col].nunique()}")
        print(f"First 5 unique values: {df_cleaned[col].unique()[:5]}")
    else:
        # For other data types, display all unique values
        print(df_cleaned[col].unique())
    print("-" * 30)

df_cleaned.describe()

df_cleaned.head()

df_cleaned.info()

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

plt.figure(figsize=(7,4))
sns.histplot(data=df_cleaned, x='computed_duration_min', bins=40, kde=True, color='teal')
plt.title("Distribution of Parking Duration (minutes)")
plt.xlabel("Computed Duration (minutes)")
plt.ylabel("Frequency")
plt.show()

categorical_cols = ['region','site','make','type','color']

for col in categorical_cols:
    plt.figure(figsize=(7,4))
    sns.countplot(data=df_cleaned, x=col, palette='pastel',
                  order=df_cleaned[col].value_counts().index)
    plt.title(f"Frequency of {col}")
    plt.xticks(rotation=45)
    plt.show()

plt.figure(figsize=(7,4))
sns.boxplot(data=df_cleaned, x='type', y='computed_duration_min', palette='coolwarm')
plt.title("Parking Duration by Vehicle Type")
plt.xlabel("Vehicle Type")
plt.ylabel("Duration (mins)")
plt.show()

plt.figure(figsize=(8,4))
avg_site = df_cleaned.groupby('site')['computed_duration_min'].mean().reset_index()
sns.barplot(data=avg_site, x='site', y='computed_duration_min', palette='viridis')
plt.title("Average Parking Duration by Site")
plt.xlabel("Parking Site")
plt.ylabel("Average Duration (mins)")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(5,4))
sns.heatmap(df_cleaned.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix of Numeric Features")
plt.show()

plt.figure(figsize=(10,5))
sns.barplot(data=df_cleaned, x='make', y='computed_duration_min',
            hue='type', ci=None, palette='Set2')
plt.title("Average Parking Duration by Vehicle Make and Type")
plt.xlabel("Vehicle Make")
plt.ylabel("Average Duration (mins)")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10,5))
sns.barplot(data=df_cleaned, x='site', y='computed_duration_min',
            hue='region', palette='muted')
plt.title("Average Parking Duration by Site and Region")
plt.xlabel("Site")
plt.ylabel("Avg Duration (mins)")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(7,4))
sns.boxplot(data=df_cleaned, x='color', y='computed_duration_min', palette='magma')
plt.title("Parking Duration by Vehicle Color")
plt.xticks(rotation=45)
plt.show()

"""# Task
Create a pie chart showing the percentage of data per lot from the dataframe `df_cleaned`.

## Group and count

### Subtask:
Group the cleaned data by `site` and `make`, and count the number of entries for each combination.

**Reasoning**:
Group the dataframe by 'site' and 'make' and count the occurrences.
"""

df_grouped = df_cleaned.groupby(['site', 'make']).size().reset_index(name='count')
display(df_grouped.head())

"""## Calculate percentage

### Subtask:
For each `site`, calculate the percentage of each `make` relative to the total number of vehicles in that site.

**Reasoning**:
Calculate the total count of vehicles per site, merge it back to the grouped dataframe, calculate the percentage, and display the head of the dataframe.
"""

site_counts = df_grouped.groupby('site')['count'].sum().reset_index(name='total_site_count')
df_grouped = pd.merge(df_grouped, site_counts, on='site')
df_grouped['percentage'] = (df_grouped['count'] / df_grouped['total_site_count']) * 100
display(df_grouped.head())

"""## Visualize

### Subtask:
Generate a pie chart for each `site` to show the distribution of car `make` percentages.

**Reasoning**:
Generate a pie chart for each site to show the distribution of car make percentages by iterating through the unique sites, filtering the data for each site, and creating a pie chart using the make and percentage columns.
"""

for site in df_grouped['site'].unique():
    site_data = df_grouped[df_grouped['site'] == site]
    plt.figure(figsize=(8, 8))
    plt.pie(site_data['percentage'], labels=site_data['make'], autopct='%1.1f%%', startangle=140)
    plt.title(f"Distribution of Car Makes at {site}")
    plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The data was grouped by `site` and `make` to count the occurrences of each combination.
*   The percentage of each `make` was calculated relative to the total number of vehicles within each `site`.
*   Pie charts were generated for each unique `site` to visualize the percentage distribution of car `make`s.

### Insights or Next Steps

*   The pie charts provide a clear visual representation of the market share of different car makes at each parking site.
*   Further analysis could involve comparing the make distribution across different sites to identify regional preferences or variations.

# Task
Analyze the provided dataset to identify factors influencing parking lot usage in Banff and build a model to predict future hourly parking demand or when a lot will be nearly full.

## Identify relevant data

### Subtask:
Determine which columns in the dataset are relevant to parking lot usage (e.g., time, duration, site).

**Reasoning**:
Identify and list the columns from the `df_cleaned` DataFrame that are relevant to analyzing parking lot usage based on the instructions.
"""

# Identify relevant columns for parking lot usage analysis
relevant_columns = [
    'entrance_datetime',
    'exit_datetime',
    'computed_duration_min',
    'site',
    'make',
    'model',
    'type',
    'color',
    'region'
]

# Print the list of relevant columns
print("Relevant columns for parking lot usage analysis:")
print(relevant_columns)